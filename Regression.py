#### Using same data used while visualization, after cleaning an all ##################

import pandas as pd
import numpy as np

np.random.seed(42)

n = 50

df = pd.DataFrame({
    'age': np.random.randint(22, 60, n),
    'salary': np.random.randint(30000, 120000, n),
    'experience': np.random.randint(0, 20, n),
    'city': np.random.choice(['Delhi', 'Mumbai', 'Bangalore', 'Chennai'], n),
    'department': np.random.choice(['IT', 'HR', 'Finance', 'Marketing'], n)
})

# Add a binary purchase variable (for logistic regression)
df['purchased'] = (df['salary'] > 70000).astype(int)

# Insert some missing values intentionally
df.loc[np.random.choice(df.index, 5), 'salary'] = np.nan
df.loc[np.random.choice(df.index, 3), 'city'] = np.nan

# Add one outlier in salary
df.loc[5, 'salary'] = 500000   # Outlier

print(df.head())



#Step A : Data Cleaning

print("Missing values:")
print(df.isnull().sum())


## Output
# Missing values:
# age           0
# salary        5
# experience    0
# city          3
# department    0
# purchased     0
# dtype: int64
#
# â­ STEP 1 â€” PROFESSIONAL / EXAM-STYLE INTERPRETATION
# (THIS is directly worth 5 marks)
# âœ” 1. salary has 5 missing values
# Bahut zyada (50 rows me se 5 missing = 10%).
# Salary numeric hai â†’ mean ya median se fill karna hoga.
# Lekin salary me ek extreme outlier (500,000) bhi hai, toh median better.
# ğŸ’¡ Exam line
#
# â€œSalary column contains 5 missing values. Since salary has outliers (e.g., 500000), median imputation is more appropriate than mean.â€
# âœ” 2. city has 3 missing values
# Categorical column
# Mode se fill karte hain
# Exam me categorical ALWAYS mode
# ğŸ’¡ Exam line:
# â€œCity column has 3 missing values. Categorical NaNs were imputed using mode.â€
# âœ” 3. age, experience, department, purchased have NO missing
# No action needed
# Safe for modeling
# age + experience â†’ ML-compatible immediately
# ğŸ’¡ Exam line:
# â€œAge, experience, department and purchased columns contain no missing values.â€
# âœ” 4. Overall dataset condition:
# Numeric missing: salary â†’ 5
# Categorical missing: city â†’ 3
# Outlier: salary (500000)
# This is exactly the kind of cleanup ML pipeline me ki jati hai.



print(df['salary'].median())

print(df['city'].mode()[0])

print(df['salary'].describe())



# Fill missing salary values
df['salary'] = df['salary'].fillna(df['salary'].median())

# Fill missing city values
df['city'] = df['city'].fillna(df['city'].mode()[0])

# Remove salary outlier
df = df[df['salary'] < 200000]

# Check summary
print(df['salary'].describe())


#****************************  SKLearn  Linear Regression ************************************

from sklearn.linear_model import LinearRegression

# Features and target
X = df[['age', 'experience']]     # inputs
y = df['salary']                  # target

# Create model
model = LinearRegression()

# Fit model
model.fit(X, y)
print('************************************************************************')
# Print learned parameters
print("Coefficients:", model.coef_)
print("Intercept:", model.intercept_)

#
# â­ YOUR OUTPUT
# Coefficients: [-325.89800111  570.28037105]
# Intercept: 80706.60073599851
#
#
# Letâ€™s break it down.
#
# â­ PART 1 â€” What is the Linear Regression formula here?
#
# Given 2 features:
#
# Xâ‚ = age
#
# Xâ‚‚ = experience
#
# Your model learned this equation:
#
# salary
# =
# âˆ’
# 325.89
# â‹…
# age
# +
# 570.28
# â‹…
# experience
# +
# 80706.60
# salary=âˆ’325.89â‹…age+570.28â‹…experience+80706.60
#
# This is your predicted salary formula.
#
# â­ PART 2 â€” Coefficient 1 â†’ Age = â€“325.89
# Meaning (baby-level):
#
# Jitni age badhti hai:
#
# â†’ Salary 325 rupees kam hoti dikh rahi hai (on average)
# â†’ Matlab â€œageâ€ salary ko aggressively increase nahi kar rahi
#
# Why this happened?
#
# Because your dataset is random â€”
# salary was NOT strongly correlated with age.
# Heatmap me bhi age â†” salary = â€“0.11 weak negative tha.
#
# Model ne usi pattern ko learn kiya.
#
# Exam wording:
#
# â€œAge has a very weak negative impact on salary (coefficient â‰ˆ â€“326).
# This indicates that age alone is not a strong predictor of salary in this dataset.â€
#
# â­ PART 3 â€” Coefficient 2 â†’ Experience = +570.28
# Meaning (baby-level):
#
# Har 1 year experience add karne se:
#
# â†’ Salary approx â‚¹570 badhne ka pattern model ne dekha.
#
# Iska matlab experience salary ka stronger predictor hai age se
# (heatmap me experience â†” salary also weak but still more than age)
#
# Exam wording:
#
# â€œExperience has a positive coefficient (~570), indicating that each extra year of experience increases expected salary by about â‚¹570.â€
#
# â­ PART 4 â€” Intercept = â‚¹80,706.60
# Meaning (simple):
#
# Agar kisi bande ki:
#
# age = 0
#
# experience = 0
#
# Toh model guess karta salary â‰ˆ â‚¹80,706.
#
# Real-life ye meaningful nahi hota, bas mathematically line ko start karne ka point hota hai.
#
# Exam wording:
#
# â€œThe intercept (~80706) represents the base salary when all features are zero. It is not directly meaningful but is part of the regression line.â€
#
# â­ PART 5 â€” Is the model good or bad?
# Based on coefficients:
#
# Age effect: tiny, almost zero â†’ weak
#
# Experience effect: small â†’ weak
#
# Salary doesnâ€™t depend much on these two in your dataset
#
# Based on heatmap:
#
# Age â†” salary = â€“0.11
# Experience â†” salary = 0.09
# â†’ Weak correlations detect ho rahe the
# So model is weak, which is OK for exam.
#
# Exam wording:
#
# â€œBoth features have weak effects. Thus, the linear regression model is not a strong predictor of salary on this dataset.â€
#
# â­ PART 6 â€” Your interpretation answer (copy-paste ready)
#
# â€œLinear regression learned the equation:
# salary = â€“325.89 Ã— age + 570.28 Ã— experience + 80706.60
#
# Age has a very small negative coefficient, showing weak negative relation with salary. Experience has a small positive coefficient (~570), indicating a mild positive impact. The intercept (~80706) represents the baseline salary.
#
# Overall the model shows weak predictive strength, which matches the correlation heatmap that showed low relationships among variables.â€
#
# This is EXACTLY what examiner wants.


#****************************  SKLearn  Linear Regression ************************************

from sklearn.linear_model import LogisticRegression
df['city_code'] = df['city'].astype('category').cat.codes
df['dept_code'] = df['department'].astype('category').cat.codes

# Prepare features and target
X = df[['age', 'experience', 'salary', 'city_code', 'dept_code']]
y = df['purchased']

# Create model
log_model = LogisticRegression(max_iter=1000)

# Fit model
log_model.fit(X, y)
print('************************************************************************')
print("Coefficients:", log_model.coef_)
print("Intercept:", log_model.intercept_)

# â­ YOUR LOGISTIC REGRESSION OUTPUT
# Coefficients:
# [-0.31333124   0.36619007   0.00633852  -0.84693227  -0.605635 ]
# Intercept: -434.43526319
#
#
# Note:
# Features ka order exactly ye hoga:
#
# age
#
# experience
#
# salary
#
# city_code
#
# dept_code
#
# So coefficient list also same order me hai.
#
# â­ PART 1 â€” Logistic Regression kya sikhta hai?
#
# It learns a formula:
#
# #p=sigmoid(a1â‹…age+a2â‹…experience+a3â‹…salary+a4.city_code+a5â‹…dept_code+b)
#
# Yani har feature ka effect purchase probability par.
#
# â­ PART 2 â€” HOW TO READ COEFFICIENTS
#
# Simple rule:
#
# âœ” Positive coefficient â†’ likelihood of 1 increases
#
# (â€œPurchased = YESâ€ hone ka chance badhta hai)
#
# âœ” Negative coefficient â†’ likelihood of 1 decreases
#
# (â€œPurchased = YESâ€ hone ka chance kam hota hai)
#
# â­ NOW LET'S READ YOUR COEFFICIENTS ONE-BY-ONE:
# âœ” 1) age coefficient = â€“0.31 (NEGATIVE)
#
# Meaning:
#
# As age increases â†’ purchase chance slightly decreases
#
# Very small effect â†’ almost no influence
#
# EXAM LINE:
#
# â€œAge has a weak negative impact on purchase probability.â€
#
# âœ” 2) experience = +0.366 (POSITIVE)
#
# Meaning:
#
# More experience â†’ higher chance of purchase
#
# But effect still mild
#
# EXAM LINE:
#
# â€œExperience has a mild positive influence on purchasing behavior.â€
#
# âœ” 3) salary coefficient = +0.0063 (SMALL POSITIVE)
#
# Salary ka effect bahut chhota lag raha hai, because value salary unit me directly enters model.
#
# BUT after scaling, even small numbers can be meaningful.
#
# EXAM LINE:
#
# â€œSalary shows a small positive relationship with purchasing likelihood.â€
#
# (Heatmap me purchased â†” salary = strong positive tha, but logistic regression unscaled features me dull ho sakta hai.)
#
# âœ” 4) city_code = â€“0.8469 (STRONG NEGATIVE)
#
# This is the strongest coefficient in the model.
#
# Meaning:
#
# City ka type purchase decision ko STRONGLY affect karta hai
#
# Kuch city groups me purchase probability low hai
#
# City category plays major role in customer behavior
#
# EXAM LINE:
#
# â€œCity_code has the strongest negative coefficient, indicating location heavily influences purchasing likelihood.â€
#
# âœ” 5) dept_code = â€“0.6056 (MODERATE NEGATIVE)
#
# Meaning:
#
# Some departments are less likely to purchase
#
# Department matters moderately
#
# EXAM LINE:
#
# â€œDepartment_code shows a moderate negative impact on purchasing probability.â€
#
# â­ PART 3 â€” WHAT ABOUT INTERCEPT?
# Intercept = -434.43
#
#
# Simple meaning:
#
# Base probability = VERY LOW
#
# Model predicts purchase = mainly if features raise the linear score significantly
#
# Exam line:
#
# â€œThe intercept is large and negative, showing low baseline purchase probability when all features are zero.â€
#
# â­ PART 4 â€” FULL EXAM INTERPRETATION (copy-paste ready)
#
# â€œLogistic regression shows that experience and salary have mild positive effects on purchase probability, while age has a weak negative impact. City_code has the strongest negative coefficient (â€“0.84), indicating location plays the most significant role in purchasing behavior. Department_code also reduces purchase likelihood moderately. The intercept (â€“434) indicates low baseline purchase probability. Thus, purchasing behavior is mostly influenced by city and department.â€
#
# This = full marks guaranteed.


#**************************************************************************************************************
#Confusion Matrix + Accuracy
from sklearn.metrics import accuracy_score, confusion_matrix

# Predict on the same dataset (for now)
y_pred = log_model.predict(X)
print('*************************************************************************')
print("Accuracy:", accuracy_score(y, y_pred))
print("Confusion Matrix:\n", confusion_matrix(y, y_pred))


#Output:

# â­ PART 1 â€” CONFUSION MATRIX (baby-level samajh)
#
# Your matrix:
#
# [[23  0]
#  [ 0 26]]
#
#
# Yeh 2Ã—2 table hota hai:
#
# 	Predicted 0	Predicted 1
# Actual 0	TN = 23	FP = 0
# Actual 1	FN = 0	TP = 26
#
# Let's decode:
#
# â­ TN = 23
#
# Actual 0 â†’ Predicted 0
# âœ” Model ne â€œNO PURCHASEâ€ sahi bola
# âœ” 23 log accurate
#
# â­ FP = 0
#
# Actual 0 â†’ Predicted 1
# âŒ Model ne â€œpurchase karegaâ€ bola but actually 0 tha
# âœ” ZERO mistakes here
#
# â­ FN = 0
#
# Actual 1 â†’ Predicted 0
# âŒ Model ne â€œpurchase nahi karegaâ€ bola but actually 1 tha
# âœ” ZERO mistakes here
#
# â­ TP = 26
#
# Actual 1 â†’ Predicted 1
# âœ” Model ne â€œpurchase yesâ€ sahi bola
#
# â­ PART 2 â€” Accuracy = 1.0 (100%)
#
# Meaning:
#
# Model ne 1 bhi case galat nahi kiya
#
# PERFECT prediction
#
# But BUT BUTâ€¦
#
# ğŸ‘‰ Yeh exam me explain karna zaroori hai:
#
# â€œSuch perfect accuracy usually indicates overfitting, because prediction was done on the same data used for training.â€
#
# Agar train-test split karoge â†’ accuracy kam hogi â†’ realistic.
#
# â­ PART 3 â€” Exam-Ready Interpretation (copy-paste)
#
# â€œThe confusion matrix shows 23 true negatives and 26 true positives, with zero false positives and zero false negatives. This results in an accuracy of 100%. Since predictions were made on the training data itself, this high accuracy likely reflects overfitting. A train-test split is required for realistic performance measurement.â€
#
# Yeh lines = Full marks.


from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix

# Split data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Fit model on training data
log_model = LogisticRegression(max_iter=1000)
log_model.fit(X_train, y_train)

# Predict on test data
y_pred_test = log_model.predict(X_test)
print('***********************************************************************')
print("Test Accuracy:", accuracy_score(y_test, y_pred_test))
print("Test Confusion Matrix:\n", confusion_matrix(y_test, y_pred_test))

#output:
#
# Aur ab main tumhe CONFUSION MATRIX + TEST ACCURACY ko ekdum beginner â†’ advanced â†’ exam topper style me samjha deta hoon.
#
# â­ YOUR TEST RESULTS
# Test Accuracy: 0.9
# Confusion Matrix:
# [[5 1]
#  [0 4]]
#
#
# Letâ€™s decode EVERYTHING step-by-step.
#
# â­ PART 1 â€” CONFUSION MATRIX ko samajhna (baby level)
#
# Confusion matrix:
#
# 	Predicted 0	Predicted 1
# Actual 0	5	1
# Actual 1	0	4
#
# Now decode:
#
# âœ” True Negatives (TN) = 5
#
# Actual 0 â†’ Predicted 0
# Model ne NO PURCHASE sahi bola
# âœ” 5 customers accurately identified as â€œnot purchasingâ€
#
# âœ” False Positives (FP) = 1
#
# Actual 0 â†’ Predicted 1
# Model ne bola customer buy karega, but actually nahi kiya
# âŒ 1 mistake here
# (ye â€œover-confident positive predictionâ€ hota hai)
#
# âœ” False Negatives (FN) = 0
#
# Actual 1 â†’ Predicted 0
# Model ne customer ko â€œnot purchaseâ€ bola, but he actually purchased
# âŒ yeh dangerous mistake hoti hai
# âœ” Par yaha 0 mistakes â†’ GOOD
#
# âœ” True Positives (TP) = 4
#
# Actual 1 â†’ Predicted 1
# 4 customers correctly predicted as â€œpurchasedâ€
#
# â­ PART 2 â€” Accuracy = 0.90 (90%)
#
# Meaning:
#
# Total test cases = 10
#
# Correct predictions = 9
#
# Wrong predictions = only 1
#
# Yeh bohot achha result hai exam ke liye.
#
# Training accuracy 100% thi
# Test accuracy 90% â†’
# Model is not overfitting much.
#
# Professor loves this point.
#
# â­ PART 3 â€” FULL EXAM-WORTHY INTERPRETATION (copy-paste ready)
#
# â€œOn the test data, the logistic regression model achieved 90% accuracy. The confusion matrix shows 5 true negatives, 4 true positives, 1 false positive, and zero false negatives. The absence of false negatives indicates the model correctly identifies all actual purchasers. Since test accuracy (90%) is lower than training accuracy (100%), the model generalizes well with mild overfitting.â€
#
# Yeh EXACT answer examiner love karta hai.
#
# â­ PART 4 â€” Practical Insight Jo Professor dekhna chahta hai
# âœ” No false negatives
#
# â†’ Model never misses a customer who will purchase
# â†’ Business-friendly result
# â†’ Good sensitivity
#
# âœ” Only 1 false positive
#
# â†’ Model ne ek galat khush banda declare kiya
# â†’ Acceptable
#
# âœ” Balanced performance
#
# â†’ Good generalization
# â†’ No high overfitting
#
# â­ PART 5 â€” What next? (Clustering time!)
#
# Now that Linear + Logistic complete:


